{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka\n",
    "Apache Kafka is an open-source distributed event streaming platform designed for building real-time data pipelines and streaming applications.Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy\n",
    "Imagine a news agency that collects stories and distributes them to various news outlets (like TV channels, websites, and newspapers).\n",
    "\n",
    "**Producers (Reporters)**:\n",
    "\n",
    "- Reporters gather stories (data) from different locations and send them to the central news agency.\n",
    "- These stories are tagged by category (e.g., politics, sports, entertainment).\n",
    "\n",
    "**Topics (Categories)**:\n",
    "\n",
    "- The news agency organizes stories into categories like politics, sports, or entertainment. These categories are like Kafka topics.\n",
    "- Each category has multiple folders (partitions) to store stories.\n",
    "\n",
    "**Brokers (News Editors)**:\n",
    "\n",
    "- The news editors manage the categories (topics), decide where stories go, and ensure they are accessible to news outlets.\n",
    "\n",
    "**Consumers (News Outlets)**:\n",
    "\n",
    "- Different outlets (TV, websites, newspapers) subscribe to categories they are interested in (e.g., a sports website subscribes to the sports category).\n",
    "- Each outlet decides when to fetch stories, and they can replay old stories if they want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## used for?\n",
    "Kafka is used to build real-time streaming data pipelines and real-time streaming applications. A data pipeline reliably processes and moves data from one system to another, and a streaming application is an application that consumes streams of data. For example, if you want to create a data pipeline that takes in user activity data to track how people use your website in real-time, Kafka would be used to ingest and store streaming data while serving reads for the applications powering the data pipeline. Kafka is also often used as a message broker solution, which is a platform that processes and mediates communication between two applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## key components\n",
    "- **topics**: A category of feed name that stores the messages (records)\n",
    "- **producers**: Publish messages to the topics\n",
    "- **brokers**: kafka servers that store the topic data and manage partitions\n",
    "- **consumers**: receives the messages and processes them\n",
    "- **consumer groups**: A set of consumers that work together to read data from Kafka topics. Each partition is assigned to only one consumer in a group (No duplication).\n",
    "- **partitions**: A topic in Kafka is divided into multiple partitions.\n",
    "Each partition is an independent, ordered log of messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how it works?\n",
    "Kafka combines two messaging models, queuing and publish-subscribe, to provide the key benefits of each to consumers\n",
    "- Producers send messages to a specific topic in Kafka.\n",
    "- Kafka stores messages in a fault-tolerant, distributed log across its brokers. A log is an ordered sequence of records, and these logs are broken up into segments, or partitions, that correspond to different subscribers.\n",
    "- Consumers read messages from topics, either in real-time or by replaying them from a specific point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture\n",
    "\n",
    "![kafka](kafka.png)\n",
    "\n",
    "- ZooKeeper is used for cluster coordination, managing metadata, leader election, and configuration in Kafka.\n",
    "- Primary broker (leader) handles all read and write requests for a partition.\n",
    "- Replica brokers (followers) maintain copies of the data and can take over as leader if the primary broker fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benefits\n",
    "**Scalable**\n",
    "- Kafkaâ€™s partitioned log model allows data to be distributed across multiple servers, making it scalable beyond what would fit on a single server. \n",
    "\n",
    "**Fast**\n",
    "- Kafka decouples data streams so there is very low latency, making it extremely fast. \n",
    "\n",
    "**Durable**\n",
    "- Partitions are distributed and replicated across many servers, and the data is all written to disk. This helps protect against server failure, making the data very fault-tolerant and durable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences vs MQ:\n",
    "- Retention: Kafka keeps data for a set amount of time, while MQ deletes messages after consumption.\n",
    "- Scalability: Kafka scales better with partitions for massive data loads.\n",
    "- Consumer Behavior: Kafka consumers can replay messages, while MQ consumers process and remove them.\n",
    "- Focus: Kafka focuses on high throughput and distributed event streaming, while MQ specializes in reliable message delivery."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
